{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Iris Specie from Sepal and Petal Length and Width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set Information: https://archive.ics.uci.edu/ml/datasets/iris\n",
    "This is perhaps the best known database to be found in the pattern recognition literature. Fisher's paper is a classic in the field and is referenced frequently to this day. (See Duda & Hart, for example.) The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from __future__ import division\n",
    "from math import e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NP softmax_with_cross_entropy Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_stochastic_creator(A_matrix):\n",
    "    m = A_matrix.shape[0]\n",
    "    sum_row_vect = np.sum(A_matrix, 1).reshape((m, 1))\n",
    "    B_matrix = A_matrix / sum_row_vect\n",
    "    return B_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(logits, labels):\n",
    "    return -np.sum(labels * np.log(logits), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_entropy(logits, labels):\n",
    "    logits = np.exp(logits)\n",
    "    softmax_array = vector_stochastic_creator(logits)\n",
    "    return cross_entropy(softmax_array, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.44342004, 1.52069407])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = np.array([[.3, .8, .1], [.1, .4, .9]])\n",
    "ex_labels = np.array([[0, 0, 1], [1, 0, 0]])\n",
    "softmax_cross_entropy(example, ex_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = np.array([[.3, .8, .1], [.1, .4, .9]])\n",
    "ex_labels = np.array([[0, 1, 0], [0, 0, 1]])\n",
    "softmax_cross_entropy(example, ex_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width        Class\n",
       "0           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "1           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "2           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "3           5.0          3.6           1.4          0.2  Iris-setosa\n",
       "4           5.4          3.9           1.7          0.4  Iris-setosa"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"iris.data.txt\")\n",
    "df.columns = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\", \"Class\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and Shuffle Data\n",
    "\n",
    "We normalize the data so that outliers do not drastically affect our learning. By making our average zero, we also make the parameter adjustment done in back propagation be weighted the same between data samples which deviate from a certain amount above and below the average. We also shuffle the data so that the model doesn't overfit to the current label it is training on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length</th>\n",
       "      <th>sepal width</th>\n",
       "      <th>petal length</th>\n",
       "      <th>petal width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.545114</td>\n",
       "      <td>-0.579025</td>\n",
       "      <td>0.753276</td>\n",
       "      <td>0.387014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.506555</td>\n",
       "      <td>0.343699</td>\n",
       "      <td>-1.349413</td>\n",
       "      <td>-1.320609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.385868</td>\n",
       "      <td>0.343699</td>\n",
       "      <td>-1.235754</td>\n",
       "      <td>-1.320609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.023809</td>\n",
       "      <td>-2.424474</td>\n",
       "      <td>-0.155995</td>\n",
       "      <td>-0.269764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.183054</td>\n",
       "      <td>-1.963112</td>\n",
       "      <td>0.128152</td>\n",
       "      <td>-0.269764</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length  sepal width  petal length  petal width Class\n",
       "0      0.545114    -0.579025      0.753276     0.387014     2\n",
       "1     -1.506555     0.343699     -1.349413    -1.320609     0\n",
       "2     -1.385868     0.343699     -1.235754    -1.320609     0\n",
       "3     -1.023809    -2.424474     -0.155995    -0.269764     1\n",
       "4      0.183054    -1.963112      0.128152    -0.269764     1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in df:\n",
    "    if column == \"Class\":\n",
    "        df[column][df[column] == 'Iris-setosa'] = 0\n",
    "        df[column][df[column] == 'Iris-versicolor'] = 1\n",
    "        df[column][df[column] == 'Iris-virginica'] = 2\n",
    "    if column != \"ID\" and column != \"Class\":\n",
    "        df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "np.round(df, 3)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into NP array and convert labels into one-hot vect\n",
    "We manipulate the true_label data into a one-hot vector so that we have a \"perfect or ideal distribution\" for our predictor which has dimensions which matches our output vector from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 4)\n",
      "(149, 3)\n"
     ]
    }
   ],
   "source": [
    "iris_array = np.asarray(df[['sepal length', 'sepal width', 'petal length', 'petal width']])\n",
    "class_array = np.asarray(df['Class'])\n",
    "# y_class = tf.constant(class_array, dtype = tf.float32, shape = [149, 1])\n",
    "num_rows = class_array.shape[0]\n",
    "num_classes = 3\n",
    "one_hot = np.zeros((num_rows, num_classes))\n",
    "rows = range(num_rows)\n",
    "cols = class_array.tolist()\n",
    "zipped = zip(rows, cols)\n",
    "for x, y in zipped:\n",
    "    one_hot[x, y] = 1\n",
    "print(iris_array.shape)\n",
    "print(one_hot.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tf.data.Dataset:\n",
    "X are Features <br>\n",
    "Y is Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = (None, 4))\n",
    "Y = tf.placeholder(tf.float32, [None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "dataset = tf.data.Dataset.batch(dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = dataset.make_initializable_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create two layer model with hidden layer size of 50\n",
    "50 was chosen because there are not many features and a large amount of hidden units are not needed. Two layers are substantial to fit a dataset with only 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = 50\n",
    "h2 = 50\n",
    "W1 = tf.Variable(tf.random_normal([4, h1]))\n",
    "b1 = tf.Variable(tf.random_normal([1, h1]))\n",
    "layer1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([h1, h2]))\n",
    "b2 = tf.Variable(tf.random_normal([1, h2]))\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, W2) + b2)\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal([h2, 3]))\n",
    "b_out = tf.Variable(tf.random_normal([1, 3]))  # TODO: Look Convexivity and RELU\n",
    "logits = tf.matmul(layer2, W_out) + b_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make loss function to reduce average of softmax_cross_entropy \n",
    "We use the negative sum for our loss function because logs of [0, 1] are negative. We want to increase the probability of the true label. Higher the probability of the true label, the lower the entroy function is, making it ideal to minimize for our loss function.\n",
    "$ H(y, p) = - \\sum_{i} y_{i} log(p_{i}) $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "loss = tf.reduce_mean(entropy) #TODO: Implement and undestand the softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(.05).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert one_hot back to true label vect and define accuracy\n",
    "Manipulate data to choose label with highest probability and define accuracy function as difference between predicted and true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = tf.nn.softmax(logits)\n",
    "y_pred_cls = tf.argmax(Y_pred,1)\n",
    "y_cls = tf.argmax(Y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred_cls, y_cls), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create feed forward structure\n",
    "Let batch size be 5 <br>\n",
    "Compute accuracy every 5 epochs <br>\n",
    "Uses tf.data.Dataset to iterate through data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "/Users/ryanleung/anaconda3/envs/py3/lib/python3.5/site-packages/tensorflow/python/data/ops/iterator_ops.py:356: UserWarning: An unusually high number of `Iterator.get_next()` calls was detected. This often indicates that `Iterator.get_next()` is being called inside a training loop, which will cause gradual slowdown and eventual resource exhaustion. If this is the case, restructure your code to call `next_element = iterator.get_next()` once outside the loop, and use `next_element` as the input to some computation that is invoked inside the loop.\n",
      "  warnings.warn(GET_NEXT_CALL_WARNING_MESSAGE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 235.1692228165455\n",
      "Epoch 1: 24.25648232473325\n",
      "Epoch 2: 8.87294908204776\n",
      "Epoch 3: 8.179501727356161\n",
      "Epoch 4: 5.63910186382331\n",
      "Epoch 5: 4.720722337679717\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 6: 5.471416684458102\n",
      "Epoch 7: 3.5911605264357362\n",
      "Epoch 8: 3.4908024283752397\n",
      "Epoch 9: 3.457334449936308\n",
      "Epoch 10: 2.470090581829009\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 11: 1.4939517586744842\n",
      "Epoch 12: 0.9606963034079854\n",
      "Epoch 13: 0.821691231827117\n",
      "Epoch 14: 0.7180675856044445\n",
      "Epoch 15: 0.7144797955162119\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 16: 0.651300773394837\n",
      "Epoch 17: 0.6335670737161934\n",
      "Epoch 18: 0.6216203599907146\n",
      "Epoch 19: 0.6445922859958841\n",
      "Epoch 20: 0.5250844906979282\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 21: 0.5486216386020146\n",
      "Epoch 22: 0.5115825595623491\n",
      "Epoch 23: 0.61553482169478\n",
      "Epoch 24: 0.5086179607915966\n",
      "Epoch 25: 0.49729455616497376\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 26: 0.4568351186800612\n",
      "Epoch 27: 0.4551624329792219\n",
      "Epoch 28: 0.4161814339269654\n",
      "Epoch 29: 0.46436872017449105\n",
      "Epoch 30: 0.3666721815814924\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 31: 0.3397443526504844\n",
      "Epoch 32: 0.30835096184247135\n",
      "Epoch 33: 0.3497377299631239\n",
      "Epoch 34: 0.32335137858416374\n",
      "Epoch 35: 0.27839097522583245\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 36: 0.29635331264440623\n",
      "Epoch 37: 0.27510175232362144\n",
      "Epoch 38: 0.2849568752294118\n",
      "Epoch 39: 0.24413409915504758\n",
      "Epoch 40: 0.2139956863732877\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 41: 0.23755764956672465\n",
      "Epoch 42: 0.1981668324186998\n",
      "Epoch 43: 0.20911766820049138\n",
      "Epoch 44: 0.1827434395181342\n",
      "Epoch 45: 0.1787146256950649\n",
      "\tVal Accuracy 1.0\n",
      "Epoch 46: 0.19483424972929697\n",
      "Epoch 47: 0.15958403321839398\n",
      "Epoch 48: 0.1759250402556063\n",
      "Epoch 49: 0.15331086787167436\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "epochs = 50\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tf.train.start_queue_runners(sess = sess)\n",
    "\n",
    "n_batches = (int) (149 / batch_size)\n",
    "for i in range(epochs):\n",
    "    total_loss = 0\n",
    "    sess.run(iter.initializer, feed_dict={ X: iris_array, Y: one_hot})\n",
    "    for batch in range(n_batches):\n",
    "        try:\n",
    "            X_batch, Y_batch = iter.get_next()\n",
    "            X_batch, Y_batch = sess.run([X_batch, Y_batch])\n",
    "            curr_batch_size = X_batch.shape[0]\n",
    "            X_batch = X_batch.reshape((curr_batch_size, 4))\n",
    "            Y_batch = Y_batch.reshape((curr_batch_size, 3))\n",
    "            o, l = sess.run([optimizer, loss], feed_dict={X: X_batch, Y: Y_batch})\n",
    "            total_loss += l\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "            \n",
    "    print(\"Epoch {0}: {1}\".format(i, total_loss))\n",
    "    if i % 5 == 0 and i!= 0:\n",
    "        try:\n",
    "            X_val, Y_val = iter.get_next()\n",
    "            X_val, Y_val = sess.run([X_val, Y_val])\n",
    "            curr_batch_size = X_val.shape[0]\n",
    "            X_val = X_val.reshape((curr_batch_size, 4))\n",
    "            Y_val = Y_val.reshape((curr_batch_size, 3))\n",
    "            val_accuracy = sess.run(accuracy, feed_dict={X: X_val, Y: Y_val})\n",
    "            print(\"\\tVal Accuracy {0}\".format(val_accuracy))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Header 1\n",
    "\n",
    "Consider the following dataset.\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "some image show here and data quality, data whiteing \n",
    "\n",
    "\n",
    "fancy math $ E = m c^2 $ \n",
    "\n",
    "$$\n",
    " E = \\frac{2}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
